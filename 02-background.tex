\section{Background and Motivation}

Vinyl has two primary motivations: to disaggregate storage performance from
capacity, and to reduce the cost of small writes. We believe that these two
combined will make for a block storage system well-suited to the cloud
environment, where customer workloads are unpredictable and everchanging, and
where most volumes are write-dominated.

\subsection{Existing approaches}

Existing work has revealed mulitple possible approaches for constructing elastic
block stores for deployment in the datacenter. The conceptually simplest is the
RADOS Block Device (RBD), a remote block device part of the Ceph storage system.
RBD is a thin layer on top of the Ceph Object Store (RADOS), and merely
translates between the two interfaces. The performance and consistency
charteristics of RBD are thus a direct reflection of the backing store.

An alternative approach, as demonstrated by the Log-Structured Virtual Disk
(LSVD) \cite{lsvd}, is to implement the block device as a log-structured device
on top of an immutable object store. The LSVD implementation is, however, mostly
a theoretical demonstration, and is not widely compatible with existing
infrastructure. It requires the installation of a custom kernel module and the
presence of high performance NVMe drives on the client, which is not always
possible in data center environments. Despite the practical drawbacks, the
performance and efficiency benefits of the LSVD approach are still sufficiently
promising to motivate further exploration.

The following examines a few existing widely-deployed approaches to elastic
block storage in more detail and discusses the tradeoffs they choose and how
they motivate our design.

\subsubsection{Ceph RBD}

The implementation of Ceph RBD is fairly simple. A block device is divided up
into fixed size chunks, typically 4MiB, and each chunk is mapped to a single
RADOS object (where RADOS objects are Ceph's mutable objects). Reads and writes
at an offset in the block device are simply translated into reads from the
corresponding RADOS object, which is possible because RADOS objects are mutable,
unlike other immutable object stores where such as scheme would be much more
difficult. Although the implementation is conceptually simple, there are quite a
number of interesting choices that effect the efficiency and performance of the
system.

Reads are fairly straightforward, and are simply translated into reads from the
backend, but this means that reads can only be cached on the OSD layer, and
not any higher layer. Every read must thus take a trip to a backend OSD, which
places a heavy burden on network bandwidth and storage device utilisation.

Writes, however, are incredibly costly. While conceptually simple, each write is
translated into a RADOS object update, which are much more complicated.  Ceph
storage pools are typically triple-replicated, and thus each write operation
must also be triple-replicated at the minimum. To maintain consistency and
durability, each write operation is further amplified into many other
operations, greatly increasing the cost of each write. This is especially true
for small writes, which nevertheless must go through the same process as large
writes. Detailed investigations\cite{lee2017understanding,aghayev2019file} have
found write amplification factors as high as 60 times for small (4KiB) writes in
Ceph, which is obviously underisable for workloads where such writes are common.

Typical deployments of RBD for virtual machines depend on hypervisor support.
The hypervisor directs reads to libRBD, which then translates I/O operations to
their corresponding Ceph RADOS operation.

\subsubsection{Salus}

Salus is built on top of the Hadoop Distributed Filesystem (HDFS) \isaac{todo}

\subsubsection{Blizzard}

\isaac{todo}

\subsection{Independence of Performance and Capacity}

Broadly speaking, storage systems have two primary characteristics: performance,
usually measured in terms of I/O operations per second (IOPS), and capacity,
the raw amount of data that can be stored. These two need not be coupled, and
we here argue that they \textit{shouldn't} be coupled. One can just as easily
have a small disk that is frequently accessed, or a large disk that is rarely
accessed.

Hardware, however, usually couples the two. A given unit of storage media,
whether SSD, HDD, persistent memory, tape, or otherwise, has a fixed amount of
capacity and performance when manufactured. Between different types of media,
the relationship between capacity and performance is often inverse, as higher
capacities are often achieved by increasing the density of the media, and thus
decreasing performance.

Existing storage systems are of course aware of this issue and actively exploit
the relationship between capacity and performance by, for example, placing
metadata and logs on high performance SSDs and data on slower HDDs instead.

This requires that the users of the system are aware of their workload
characteristics ahead of time and plan for it, which while possible in limited
circumstances, is generally not possible in general cloud environments where
user workloads are arbitrary and unknowable ahead of time. Cloud providers would
ideally offer the ability to provision capacity and performance separately, as
usage of one does not imply the other.


\subsection{Write Dominance}

To understand why writes are so costly in Ceph RBD, we must first understand
the path of a single write operation, regardless of its size.

The RBD client first determines, using a consistent hash, the OSD to which the
operation should be sent via a consistent hash. The OSD, upon receiving the
request, must determine how to execute said request. If the data is written to a
new object, it simply allocates space, writes the data, adds the location of the
data to the metadata store (RocksDB), and returns. If the data is a partial
overwrite, however, to prevent data loss due to power failure, the OSD must
first write the data to a write-ahead log before commiting the operation. All
the above is repeated on two other OSDs, and the operation only succeeds if
all three OSDs succeed in their write.

This means that, in the worst case, a single write operation can be amplified
many times over in the backend. This is especially true of small writes due to
minimum allocation sizes present throughout the stack, and thus amplyfing the
write operations even further.

Previous versions of Ceph used local filesystems to store objects, which meant
ever worse write amplification characteristics as each write first gets written
to a write-ahead log, then to the filesystem, resulting in doubled writes.
Recent versions of Ceph bypass the local filesystem and instead directly write
to the underlying block devices, achieving much better write performance and
efficiency \cite{aghayev2019file}.

Most problematically, existing investigation into the nature of block storage
workloads have found that small writes are not an insignificant portion of most
cloud environments, and that most volumes are write-dominant in nature, as reads
are commonly served from cache and thus reach the backend less frequently.  Li
et al \cite{li2020depth} found small writes dominate both AliCloud and
Microsoft's Research Cloud block traces, where the average write size was less
than 16KiB.  Additionally, write workloads are bursty, with many small writes
occuring in a small space of time instead of a more even spread of requests over
time.

The above observations motivate a search for an approach that can reduce the
overhead of writes, especially small writes, while still maintaining the same
consistency and durability guarantees of the original system. This is especially
important as small writes are extremely common in many block storage workloads,
and thus run into the above inefficient write path frequently.

\isaac{explain architecture of lsvd here? or in architecture}

\subsection{Log-Structured Storage}

Log-structured systems have been studied extensively in the past. The need for
good performance has remained ever since the first log-structured file system
was designed, and is the primary motivation for our work.

To ensure durability of data, the write paths for conventional remote storage
systems typically involve multiple writes to the backend, typically including
one or more writes to a journal, followed by metadata, and finally the data
itself to the backend. When summed across multiple replicated copies of the
data, the amplification factor can be quite high. This makes small-write
workloads particularly inefficient, as the overhead of the writes can be
dwarfed by the overhead of replication, journaling, and in ensuring consistency.

LSVD showed that a log-structured approach can be used to dramatically reduce
the overhead of writes by batching writes together and only occasionally
flushing them to the backend. LSVD, however, requires a high-performance local
drive for write journaling and for read caching, which is not always available
in datacenter environments.

We believe that the log-structured approach is sound, and thus built an
implementation where the system is moved to an intermediate gateway. This
reduces the requirement of hardware from every single client machine to only the
few gateway servers, which is much more doable in a datacenter environment.

An additional benefit of representing backend data as discrete log objects, as
in LSVD, is that the backend representation of the disk is always consistent.
Other approaches may sometime leave the durable representation of the disk in an
inconsistent or invalid state, requiring additional work on failure to recover
to a consistent state.

\subsection{Storage Placement Tradeoffs}

Users of a storage system typically demand that data, above all, is
\textit{safe}. This means that no amount of data loss is acceptable, and thus
the system must ensure that a write, once acknowledged, is resilient to the
required failure scenarios.

Methods of accomplishing the above are manyfold and extensively studied in
existing literature. Broadly speaking, there are three possible locations for
the processing of storage data: on the client, on a gateway, directly on the
backend, or a combination of the three.

Putting the processing on the client, as with the Blizzard and LSVD, is likely
the simplest approach, but has the downside of often requiring explicit client
support. This also allows prevents sharing of resources between different
clients not on the same host as they are physically unable to do so.

Maintaining crash consistency in the event of failure, however, is tricky. If
the write journal is placed on the same host as the client, then failure of the
client host will lose data. The alternative is to not acknowledge writes until
they're committed to the backend, but this is a costly approach as explained in
the previously.

By co-locating state with the client, we also lose the ability to easily migrate
virtual machines to another machine when required without additional complexity,
either in also migrating state or by redirecting the client to the old host
to access previous state.

The upside, however, is that the client exists in the same failure domain as
the storage system, and thus the failure of one almost certainly implies the
other. \isaac{todo}

\subsection{Other Implementations}

\isaac{what do we know about other elastic block implementations? ??? in AWS
	EBS, something FPGA in Azure, ??? in GCP}
