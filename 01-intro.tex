\section{Introduction}

Remote block storage devices are the foundation of many cloud environments as a
simple, widely-compatible source of persistent storage. As such, any
implementation must contend with a wide range of workloads and use cases,
providing similar semantics and performance with traditional local storage.
Many cloud providers provide an implementation of an elastic block store system,
but none of them have been open-sourced or published.

Existing work, however, hes revealed mulitple possible approaches for
constructing elastic block stores for deployment in the datacenter. The most
notable is the RADOS Block Device (RBD), a remote block device part of the Ceph
storage system. RBD is a thin layer on top of the Ceph Object Store (RADOS), and
merely translates between the two interfaces. The performance and consistency
charteristics of RBD are thus a direct reflection of the backing store.

An alternative approach, as demonstrated by the Log-Structured Virtual Disk
(LSVD) \cite{lsvd}, is to implement the block device as a log-structured device
on top of an immutable object store. The LSVD implementation is, however, mostly
a theoretical demonstration, and is not widely compatible with existing
infrastructure. It requires the installation of a custom kernel module and the
presence of high performance NVMe drives on the client, which is not always
possible in data center environments. Despite the practical drawbacks, the
performance and efficiency benefits of the LSVD approach are still sufficiently
promising to motivate further exploration.

With this in mind, we propose \isaac{placeholder name} \textbf{Vinyl}, a
concrete datacenter-ready implementation of a log-structured remote block
storage device. We move the processing onto a central storage gateway server
separate from the client, and show that our approach retains all the semantics
and performance of the original LSVD implementation, while also engineering it
for deployment in the datacenter.

We demonstrate that a centralised gateway instead of a client-side
implementation brings with it many benefits:

\begin{enumerate}

  \item \textbf{Client compatibility:} the gateway can be deployed as a drop-in
        replacement for existing block devices, and does not require any modifications
        to the client. We can expose our gateway as a standard NVMe-oF or iSCSI
        device, thus providing broad compatibility with most existing clients in a
        datacenter environment with no modifications required.

  \item \textbf{Deployment compatibility:} by not requiring any client
        modifications and only minimal hardware support on the gateway server, our
        implementation easily fits into exsiting datacenter environments. Most
        server environments have fixed hardware configurations, and thus cannot
        accodomate any design which requires hardware configurations uncommon
        in the datacenter.

  \item \textbf{Statistical multiplexing} between clients, allowing for higher
        utilisation of datacenter resources.

  \item \textbf{Shared cache:} The gateway can cache data from multiple clients
        and thus amortise the cost of a read across multiple clients. This is
        especially useful for workloads with high locality, such as booting a cluster
        of identical virtual machines, where many of the blocks are shared and thus
        take up no additional cache space per client (described later in the
        architecture details).

\end{enumerate}

Our prototype demonstrates read performance on par with that of a local SSD for
workloads that fit within our local cache while maintaining the benefit of
improved write performance and efficiency.

We additionally explore the tradeoffs inherent in the choice of placement of
the storage service, and show that the gateway approach is a viable alternative
to purely client or backend-side implementations.

There is a large body of existing literature log-structured storage systems,
storage gateways, and remote block storage systems. Our implementation differs
in the unique combination of all three in a single system, and in the practical
implementation that is suited for datacenter environments. We also discuss the
design choices made during the implementation, and the resulting semantics and
performances tradeoff that are the result of said choices.

We primarily evaluate against Ceph RBD as a widely used remote block storage
system, and show that our implementation far exceeds RBD for writes while
and also reads when the working set fits within the local cache, achieving
near-local SSD performance.

We show that a log-structured block store built on immutable objects is viable
for datacenter deployment with a centralised gateway, and that the expected
performance characteristics of such a system are acheivable in practice.
