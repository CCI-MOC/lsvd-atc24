\section{Introduction}

Remote block storage devices are the foundation of many cloud environments as a
simple, widely-compatible source of persistent storage. As such, any
implementation must contend with a wide range of workloads and use cases,
providing similar semantics and performance with traditional local storage.
Many cloud providers provide an implementation of an elastic block store system,
but none of them have been open-sourced or published.

Existing approaches, however, tie together two separate characteristics of a
storage system: capacity and performance (i.e. IOPS). RBD is a thin layer on the
backend storage servers, and thus IOPS is fundamentally limited by the hardware
already deployed on said servers. LSVD runs on the client, and is thus limited
by client hardware and not elastic in practice. This is at odds with the modern
approach to building datacenters which provisions and plans for all hardware and
capacity up-front, and thus cannot accomodate changes in hardware during the
service life.

We thus propose \textbf{Vinyl} \isaac{(placeholder name)}, a datacenter-suited
implementation of a log-structured remote block storage device. We argue that
the separation of storage capacity performance it iteslf desirable, and show
that our implementation is able to achieve or exceed the performance and
efficiency of existing remote block storage systems while also being elastic
in performance independently of capacity or client hardware. We retain the

We show that by moving storage processing onto an intermediate gateway server
instead of being on the client or backend, we achieve the following:

\begin{enumerate}

    \item \textbf{Elastic performance:} the gateway server can be scaled
          independently of the backend storage servers, allowing for elastic
          performance without requiring any changes to the backend storage servers or
          the clients.  This is especially useful in datacenter environments where
          hardware is provisioned up-front and cannot be changed during the service
          life.

    \item \textbf{Client compatibility:} the gateway can be deployed as a drop-in
          replacement for existing block devices, and does not require any modifications
          to the client. We can expose our gateway as a standard NVMe-oF or iSCSI
          device, thus providing broad compatibility with most existing clients in a
          datacenter environment with no modifications required.

    \item \textbf{Deployment compatibility:} by not requiring any client
          modifications and only minimal hardware support on the gateway server, our
          implementation easily fits into exsiting datacenter environments. Most
          server environments have fixed hardware configurations, and thus cannot
          accodomate any design which requires hardware configurations uncommon
          in the datacenter.

    \item \textbf{Client resource sharing:} the gateway serves multiple clients
          simultaneously, thus sharing gateway resources across all clients it serves.
          Of special note is the sharing of the cache, which is especially helpful in
          workloads with high locality (such as booting a cluster of virtual
          machines), where many of the blocks are shared and thus take up no
          additional cache space per client served.

\end{enumerate}

Our prototype demonstrates read performance on par with that of a local SSD for
workloads that fit within our local cache while maintaining the benefit of
improved write performance and efficiency.

We additionally explore the tradeoffs inherent in the choice of placement of
the storage service, and show that the gateway approach is a viable alternative
to purely client or backend-side implementations.

There is a large body of existing literature log-structured storage systems,
storage gateways, and remote block storage systems. Our implementation differs
in the unique combination of all three in a single system, and in the practical
implementation that is suited for datacenter environments. We also discuss the
design choices made during the implementation, and the resulting semantics and
performances tradeoff that are the result of said choices.

We primarily evaluate against Ceph RBD as a widely used remote block storage
system, and show that our implementation far exceeds RBD for writes while
and also reads when the working set fits within the local cache, achieving
near-local SSD performance.

We show that a log-structured block store built on immutable objects is viable
for datacenter deployment with an intermediate gateway server, that the expected
performance characteristics of such a system are acheivable in practice, and
that said performance is independently scalable of the backend or client.
